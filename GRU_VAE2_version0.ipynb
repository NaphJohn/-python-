{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU-VAE2-version0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0ABcz/6QLkMwBzsN2N7Ad",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whkaikai/-python-/blob/main/GRU_VAE2_version0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "q6Xe0sMkhwAA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "keE4MBzfhIOw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/test0.csv')\n",
        "train_data1 = data[data['SPLIT'] == 'train']\n",
        "train_data_smiles2 = (train_data1[\"SMILES\"].squeeze()).astype(str).tolist()\n",
        "train_data = train_data_smiles2\n",
        "\n",
        "\n",
        "#Constructing a vocabulary\n",
        "chars = set()\n",
        "for string in train_data:\n",
        "    chars.update(string)\n",
        "all_sys = sorted(list(chars)) + ['<bos>', '<eos>', '<pad>', '<unk>']\n",
        "vocab = all_sys\n",
        "c2i = {c: i for i, c in enumerate(all_sys)}\n",
        "i2c = {i: c for i, c in enumerate(all_sys)}\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vector = torch.eye(len(c2i))\n",
        "\n",
        "\n",
        "def char2id(char):\n",
        "    if char not in c2i:\n",
        "        return c2i['<unk>']\n",
        "    else:\n",
        "        return c2i[char]\n",
        "\n",
        "\n",
        "def id2char(id):\n",
        "    if id not in i2c:\n",
        "        return i2c[32]\n",
        "    else:\n",
        "        return i2c[id]\n",
        "\n",
        "def string2ids(string,add_bos=False, add_eos=False):\n",
        "    ids = [char2id(c) for c in string]\n",
        "    if add_bos:\n",
        "        ids = [c2i['<bos>']] + ids\n",
        "    if add_eos:\n",
        "        ids = ids + [c2i['<eos>']]\n",
        "    return ids\n",
        "def ids2string(ids, rem_bos=True, rem_eos=True):\n",
        "    if len(ids) == 0:\n",
        "        return ''\n",
        "    if rem_bos and ids[0] == c2i['<bos>']:\n",
        "        ids = ids[1:]\n",
        "    if rem_eos and ids[-1] == c2i['<eos>']:\n",
        "        ids = ids[:-1]\n",
        "    string = ''.join([id2char(id) for id in ids])\n",
        "    return string\n",
        "def string2tensor(string, device='model'):\n",
        "    ids = string2ids(string, add_bos=True, add_eos=True)\n",
        "    tensor = torch.tensor(ids, dtype=torch.long,device=device if device == 'model' else device)\n",
        "    return tensor\n",
        "tensor = [string2tensor(string, device=device) for string in train_data]\n",
        "vector = torch.eye(len(c2i))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE Model"
      ],
      "metadata": {
        "id": "39X79ZxGkDfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "q_bidir = True\n",
        "q_d_h = 256\n",
        "q_n_layers = 1\n",
        "q_dropout = 0.5\n",
        "d_n_layers = 3\n",
        "d_dropout = 0\n",
        "d_z = 128\n",
        "d_d_h = 512\n",
        "\n",
        "class VAE(nn.Module):\n",
        "  def __init__(self,vocab,vector):\n",
        "    super().__init__()\n",
        "    self.vocabulary = vocab\n",
        "    self.vector = vector\n",
        "    \n",
        "    n_vocab, d_emb = len(vocab), vector.size(1)\n",
        "    self.x_emb = nn.Embedding(n_vocab, d_emb, c2i['<pad>'])\n",
        "    self.x_emb.weight.data.copy_(vector)\n",
        "  \n",
        "    #ENCODER\n",
        "    \n",
        "    self.encoder_rnn = nn.GRU(d_emb,q_d_h,num_layers=q_n_layers,batch_first=True,dropout=q_dropout if q_n_layers > 1 else 0,bidirectional=q_bidir)\n",
        "    q_d_last = q_d_h * (2 if q_bidir else 1)\n",
        "    self.q_mu = nn.Linear(q_d_last, d_z)\n",
        "    self.q_logvar = nn.Linear(q_d_last, d_z)\n",
        "  \n",
        "  \n",
        "  \n",
        "    # Decoder\n",
        "    self.decoder_rnn = nn.GRU(d_emb + d_z,d_d_h,num_layers=d_n_layers,batch_first=True,dropout=d_dropout if d_n_layers > 1 else 0)\n",
        "    self.decoder_latent = nn.Linear(d_z, d_d_h)\n",
        "    self.decoder_fullyc = nn.Linear(d_d_h, n_vocab)\n",
        "  \n",
        "  \n",
        "  \n",
        "    # Grouping the model's parameters\n",
        "    self.encoder = nn.ModuleList([self.encoder_rnn,self.q_mu,self.q_logvar])\n",
        "    self.decoder = nn.ModuleList([self.decoder_rnn,self.decoder_latent,self.decoder_fullyc])\n",
        "    self.vae = nn.ModuleList([self.x_emb,self.encoder,self.decoder])\n",
        "    \n",
        "    \n",
        "    \n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def string2tensor(self, string, device='model'):\n",
        "    ids = string2ids(string, add_bos=True, add_eos=True)\n",
        "    tensor = torch.tensor(ids, dtype=torch.long,device=self.device if device == 'model' else device)\n",
        "    return tensor\n",
        "\n",
        "  def tensor2string(self, tensor):\n",
        "    ids = tensor.tolist()\n",
        "    string = ids2string(ids, rem_bos=True, rem_eos=True)\n",
        "    return string\n",
        "  \n",
        "  def forward(self,x):\n",
        "    z, kl_loss = self.forward_encoder(x)\n",
        "    recon_loss = self.forward_decoder(x, z)\n",
        "    print(\"forward\")\n",
        "    return kl_loss, recon_loss\n",
        "  \n",
        "  def forward_encoder(self,x):\n",
        "    x = [self.x_emb(i_x) for i_x in x]\n",
        "    x = nn.utils.rnn.pack_sequence(x)\n",
        "    _, h = self.encoder_rnn(x, None)\n",
        "    h = h[-(1 + int(self.encoder_rnn.bidirectional)):]\n",
        "    h = torch.cat(h.split(1), dim=-1).squeeze(0)\n",
        "    mu, logvar = self.q_mu(h), self.q_logvar(h)\n",
        "    eps = torch.randn_like(mu)\n",
        "    z = mu + (logvar / 2).exp() * eps\n",
        "    kl_loss = 0.5 * (logvar.exp() + mu ** 2 - 1 - logvar).sum(1).mean()\n",
        "    return z, kl_loss\n",
        "  \n",
        "  def forward_decoder(self,x, z):\n",
        "    lengths = [len(i_x) for i_x in x]\n",
        "    x = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value= c2i['<pad>'])\n",
        "    x_emb = self.x_emb(x)\n",
        "    z_0 = z.unsqueeze(1).repeat(1, x_emb.size(1), 1)\n",
        "    x_input = torch.cat([x_emb, z_0], dim=-1)\n",
        "    x_input = nn.utils.rnn.pack_padded_sequence(x_input, lengths, batch_first=True)\n",
        "    h_0 = self.decoder_latent(z)\n",
        "    h_0 = h_0.unsqueeze(0).repeat(self.decoder_rnn.num_layers, 1, 1)\n",
        "    output, _ = self.decoder_rnn(x_input, h_0)\n",
        "    output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
        "    y = self.decoder_fullyc(output)\n",
        "    \n",
        "    recon_loss = F.cross_entropy(y[:, :-1].contiguous().view(-1, y.size(-1)),x[:, 1:].contiguous().view(-1),ignore_index= c2i['<pad>'])\n",
        "    return recon_loss\n",
        "  \n",
        "    \n",
        "  def sample_z_prior(self,n_batch):\n",
        "    return torch.randn(n_batch,self.q_mu.out_features,device= self.x_emb.weight.device)\n",
        "  def sample(self,n_batch, max_len=100, z=None, temp=1.0):\n",
        "    with torch.no_grad():\n",
        "      if z is None:\n",
        "        z = self.sample_z_prior(n_batch)\n",
        "        z = z.to(self.device)\n",
        "        z_0 = z.unsqueeze(1)\n",
        "        h = self.decoder_latent(z)\n",
        "        h = h.unsqueeze(0).repeat(self.decoder_rnn.num_layers, 1, 1)\n",
        "        w = torch.tensor(c2i['<bos>'], device=self.device).repeat(n_batch)\n",
        "        x = torch.tensor([c2i['<pad>']], device=device).repeat(n_batch, max_len)\n",
        "        x[:, 0] = c2i['<bos>']\n",
        "        end_pads = torch.tensor([max_len], device=self.device).repeat(n_batch)\n",
        "        eos_mask = torch.zeros(n_batch, dtype=torch.uint8, device=self.device)\n",
        "\n",
        "\n",
        "        for i in range(1, max_len):\n",
        "          x_emb = self.x_emb(w).unsqueeze(1)\n",
        "          x_input = torch.cat([x_emb, z_0], dim=-1)\n",
        "\n",
        "          o, h = self.decoder_rnn(x_input, h)\n",
        "          y = self.decoder_fullyc(o.squeeze(1))\n",
        "          y = F.softmax(y / temp, dim=-1)\n",
        "\n",
        "          w = torch.multinomial(y, 1)[:, 0]\n",
        "          x[~eos_mask, i] = w[~eos_mask]\n",
        "          i_eos_mask = ~eos_mask & (w == c2i['<eos>'])\n",
        "          end_pads[i_eos_mask] = i + 1\n",
        "          eos_mask = eos_mask | i_eos_mask\n",
        "          \n",
        "          \n",
        "          new_x = []\n",
        "          for i in range(x.size(0)):\n",
        "            new_x.append(x[i, :end_pads[i]])\n",
        "\n",
        "            \n",
        "    return [self.tensor2string(i_x) for i_x in new_x]"
      ],
      "metadata": {
        "id": "9iN7xAxGjlty"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainging"
      ],
      "metadata": {
        "id": "8xgYk4sSkO6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import UserList, defaultdict\n",
        "n_last = 1000\n",
        "n_batch = 32\n",
        "kl_start = 0\n",
        "kl_w_start = 0.0\n",
        "kl_w_end = 1.0\n",
        "n_epoch = 50\n",
        "n_workers = 0\n",
        "\n",
        "clip_grad  = 50\n",
        "lr_start = 0.003\n",
        "lr_n_period = 10\n",
        "lr_n_mult = 1\n",
        "lr_end = 3 * 1e-4\n",
        "lr_n_restarts = 6\n",
        "\n",
        "\n",
        "def _n_epoch():\n",
        "    return sum(lr_n_period * (lr_n_mult ** i) for i in range(lr_n_restarts))\n",
        "  \n",
        "def _train_epoch(model, epoch, train_loader, kl_weight, optimizer=None):\n",
        "    if optimizer is None:\n",
        "        model.eval()\n",
        "    else:\n",
        "        model.train()\n",
        "      \n",
        "    kl_loss_values = CircularBuffer(n_last)\n",
        "    recon_loss_values = CircularBuffer(n_last)\n",
        "    loss_values = CircularBuffer(n_last)\n",
        "    for i, input_batch in enumerate(train_loader):\n",
        "        input_batch = tuple(data.to(device) for data in input_batch)\n",
        "      \n",
        "    #forward\n",
        "        kl_loss, recon_loss = model(input_batch)\n",
        "        loss = kl_weight * kl_loss + recon_loss\n",
        "    #backward\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(get_optim_params(model),clip_grad)\n",
        "            optimizer.step()\n",
        "      \n",
        "        kl_loss_values.add(kl_loss.item())\n",
        "        recon_loss_values.add(recon_loss.item())\n",
        "        loss_values.add(loss.item())\n",
        "        lr = (optimizer.param_groups[0]['lr'] if optimizer is not None else None)\n",
        "      \n",
        "    #update train_loader\n",
        "        kl_loss_value = kl_loss_values.mean()\n",
        "        recon_loss_value = recon_loss_values.mean()\n",
        "        loss_value = loss_values.mean()\n",
        "        postfix = [f'loss={loss_value:.5f}',f'(kl={kl_loss_value:.5f}',f'recon={recon_loss_value:.5f})',f'klw={kl_weight:.5f} lr={lr:.5f}']\n",
        "    postfix = {'epoch': epoch,'kl_weight': kl_weight,'lr': lr,'kl_loss': kl_loss_value,'recon_loss': recon_loss_value,'loss': loss_value,'mode': 'Eval' if optimizer is None else 'Train'}\n",
        "    return postfix\n",
        "  \n",
        "def _train(model, train_loader, val_loader=None, logger=None):\n",
        "    optimizer = optim.Adam(get_optim_params(model),lr= lr_start)\n",
        "    \n",
        "    lr_annealer = CosineAnnealingLRWithRestart(optimizer)\n",
        "    \n",
        "    model.zero_grad()\n",
        "    for epoch in range(n_epoch):\n",
        "      \n",
        "        kl_annealer = KLAnnealer(n_epoch)\n",
        "        kl_weight = kl_annealer(epoch)\n",
        "        postfix = _train_epoch(model, epoch,train_loader, kl_weight, optimizer)\n",
        "        lr_annealer.step()\n",
        "def fit(model, train_data, val_data=None):\n",
        "    logger = Logger() if False is not None else None\n",
        "    train_loader = get_dataloader(model,train_data,shuffle=True)\n",
        "\n",
        "    \n",
        "    \n",
        "    val_loader = None if val_data is None else get_dataloader(model, val_data, shuffle=False)\n",
        "    _train(model, train_loader, val_loader, logger)\n",
        "    return model\n",
        "def get_collate_device(model):\n",
        "    return model.device\n",
        "def get_dataloader(model, train_data, collate_fn=None, shuffle=True):\n",
        "    if collate_fn is None:\n",
        "        collate_fn = get_collate_fn(model)\n",
        "        print(collate_fn)\n",
        "    return DataLoader(train_data, batch_size=n_batch, shuffle=shuffle, num_workers=n_workers, collate_fn=collate_fn)\n",
        "\n",
        "def get_collate_fn(model):\n",
        "    device = get_collate_device(model)\n",
        "\n",
        "    def collate(train_data):\n",
        "        train_data.sort(key=len, reverse=True)\n",
        "        tensors = [string2tensor(string, device=device) for string in train_data]\n",
        "        return tensors\n",
        "\n",
        "    return collate\n",
        "\n",
        "def get_optim_params(model):\n",
        "    return (p for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "class KLAnnealer:\n",
        "    def __init__(self,n_epoch):\n",
        "        self.i_start = kl_start\n",
        "        self.w_start = kl_w_start\n",
        "        self.w_max = kl_w_end\n",
        "        self.n_epoch = n_epoch\n",
        "\n",
        "        \n",
        "        self.inc = (self.w_max - self.w_start) / (self.n_epoch - self.i_start)\n",
        "\n",
        "    def __call__(self, i):\n",
        "        k = (i - self.i_start) if i >= self.i_start else 0\n",
        "        return self.w_start + k * self.inc\n",
        "      \n",
        "      \n",
        "      \n",
        "class CosineAnnealingLRWithRestart(_LRScheduler):\n",
        "    def __init__(self , optimizer):\n",
        "        self.n_period = lr_n_period\n",
        "        self.n_mult = lr_n_mult\n",
        "        self.lr_end = lr_end\n",
        "\n",
        "        self.current_epoch = 0\n",
        "        self.t_end = self.n_period\n",
        "\n",
        "        # Also calls first epoch\n",
        "        super().__init__(optimizer, -1)\n",
        "\n",
        "    def get_lr(self):\n",
        "        return [self.lr_end + (base_lr - self.lr_end) *\n",
        "                (1 + math.cos(math.pi * self.current_epoch / self.t_end)) / 2\n",
        "                for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "        self.last_epoch = epoch\n",
        "        self.current_epoch += 1\n",
        "\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        if self.current_epoch == self.t_end:\n",
        "            self.current_epoch = 0\n",
        "            self.t_end = self.n_mult * self.t_end\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "class CircularBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.max_size = size\n",
        "        self.data = np.zeros(self.max_size)\n",
        "        self.size = 0\n",
        "        self.pointer = -1\n",
        "\n",
        "    def add(self, element):\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.pointer = (self.pointer + 1) % self.max_size\n",
        "        self.data[self.pointer] = element\n",
        "        return element\n",
        "\n",
        "    def last(self):\n",
        "        assert self.pointer != -1, \"Can't get an element from an empty buffer!\"\n",
        "        return self.data[self.pointer]\n",
        "\n",
        "    def mean(self):\n",
        "        return self.data.mean()\n",
        "      \n",
        "      \n",
        "class Logger(UserList):\n",
        "    def __init__(self, data=None):\n",
        "        super().__init__()\n",
        "        self.sdata = defaultdict(list)\n",
        "        for step in (data or []):\n",
        "            self.append(step)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, int):\n",
        "            return self.data[key]\n",
        "        elif isinstance(key, slice):\n",
        "            return Logger(self.data[key])\n",
        "        else:\n",
        "            ldata = self.sdata[key]\n",
        "            if isinstance(ldata[0], dict):\n",
        "                return Logger(ldata)\n",
        "            else:\n",
        "                return ldata\n",
        "\n",
        "    def append(self, step_dict):\n",
        "        super().append(step_dict)\n",
        "        for k, v in step_dict.items():\n",
        "            self.sdata[k].append(v)\n"
      ],
      "metadata": {
        "id": "BrvfZvnZkLDh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5b0wf9d7knPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample from model"
      ],
      "metadata": {
        "id": "O283jUxukjuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "n_samples = 30\n",
        "n_jobs = 1\n",
        "max_len = 100\n",
        "\n",
        "class sample():\n",
        "  def take_samples(model,n_batch):\n",
        "    n = n_samples\n",
        "    samples = []\n",
        "    with tqdm(total=n_samples, desc='Generating samples') as T:\n",
        "      while n > 0:\n",
        "        current_samples = model.sample(min(n, n_batch), max_len)\n",
        "        samples.extend(current_samples)\n",
        "        n -= len(current_samples)\n",
        "        T.update(len(current_samples))\n",
        "    samples = pd.DataFrame(samples, columns=['SMILES'])\n",
        "    return samples"
      ],
      "metadata": {
        "id": "AyR-9dQoknfU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gensamples"
      ],
      "metadata": {
        "id": "HyH_m1swkuuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAE(vocab,vector).to(device)\n",
        "fit(model, train_data)\n",
        "model.eval()\n",
        "sample = sample.take_samples(model,n_batch)\n",
        "print(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV4urvOwk3CU",
        "outputId": "778bee82-8788-4f1c-fbf5-7737a91f0b3a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function get_collate_fn.<locals>.collate at 0x7f3012b1a950>\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n",
            "forward\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating samples:   0%|          | 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:123: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:125: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1581.)\n",
            "Generating samples: 100%|██████████| 3000/3000 [01:17<00:00, 38.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 SMILES\n",
            "0                                   C=CCOnnlnc1()=NcOc=\n",
            "1                      n11CccC1c1C=CCc(2=l)C=)=()))C=On\n",
            "2     2C(CSCON<unk>lCllc(1lcCnO1C(O]OlO(=1HO)H)1)ncH...\n",
            "3                                        CCCC21ccOc)c11\n",
            "4                                                      \n",
            "...                                                 ...\n",
            "2995                       CCCcCc(2O2c)cc1c)N())cccC)2c\n",
            "2996                                              C1(1=\n",
            "2997                                       1))1ccOc=H1c\n",
            "2998                         [Cc2H]1c12)12n((=)(c(Ccc1n\n",
            "2999              C11OcCcncc2OOOn)c=)1OOOc2CCc1lO=cHcc)\n",
            "\n",
            "[3000 rows x 1 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}